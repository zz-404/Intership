1. tokenizer 
2. embedding
    将token和高维向量发生关联，这个过程称为embedding
    - embedding matrix 12 k * 50 k (50k个单词，12k个语义维度)
    - T 温度，利用e的特性放大或缩小极端值的的占比，使得分布更加不平衡或均匀
3. Attention
   - 词嵌入层只嵌入含义以及编码位置，并不更新上下文语义
   - 把所有嵌入向量相关联，更新最后一个向量，且只使用最后一个向量来做预测
   - 单头注意力机制
     - 设置查询矩阵，与嵌入向量相乘得到查询向量，查询矩阵都是通过一个 120 * 12k
     - 设置键矩阵，与嵌入向量相乘得到键向量，类似于回答查询向量的答案 120 * 12k
     - 将二维矩阵中的每个查询向量和键向量做点积，得到对齐程度，对齐程度较高的就嵌入
     - 嵌入过程，每个向量：根据对齐程度对所有其他向量做softmax
       
       $softmax( Q K^{T} / dk^{1/2} )$
  
       除以维度的平方根是为了平滑概率分布，相当于温度
  
       - 掩码：为了不让后面的token影响前面的token，相当于透露答案，我们需要先让矩阵的左下角全部为负无穷，接着softmax归一化之后再得到对
     - 值矩阵 value矩阵，与嵌入向量相乘决定对实际改变值的影响度，最后再和上面的softmax处理后的对齐程度相乘，得到ΔE，再将ΔE更新嵌入向量 12k*12k
       **改进的方法，想让值矩阵的参数量等于查询矩阵和键矩阵的参数之和**
     - 
     - 
   - 
5. unembedding matrix 50 k *12 k
